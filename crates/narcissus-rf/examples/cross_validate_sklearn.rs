//! Cross-validate narcissus-rf against scikit-learn (Python) reference data.
//!
//! Reads `cross_val/rf_reference.json` (generated by `cross_val/generate_rf_references.py`)
//! and compares our Random Forest results against scikit-learn's ground truth.
//!
//! Run:
//!   cargo run --example cross_validate_sklearn -p narcissus-rf
//!
//! Generate reference data first:
//!   cd cross_val && uv run generate_rf_references.py

use std::fs;

use serde::Deserialize;

use narcissus_rf::{CrossValidation, MaxFeatures, OobMode, RandomForestConfig};

// ── JSON schema matching rf_reference.json ──────────────────────────────

#[derive(Deserialize)]
struct Reference {
    dataset: DatasetRef,
    sklearn_rf: SklearnRf,
    sklearn_cv: SklearnCv,
}

#[derive(Deserialize)]
struct DatasetRef {
    features: Vec<Vec<f64>>,
    labels: Vec<usize>,
    feature_names: Vec<String>,
    n_samples: usize,
    n_features: usize,
    n_classes: usize,
}

#[derive(Deserialize)]
struct SklearnRf {
    predictions: Vec<usize>,
    feature_importances: Vec<f64>,
    importance_ranking: Vec<usize>,
    train_accuracy: f64,
}

#[derive(Deserialize)]
struct SklearnCv {
    n_folds: usize,
    mean_accuracy: f64,
    std_accuracy: f64,
    #[allow(dead_code)]
    fold_accuracies: Vec<f64>,
    #[allow(dead_code)]
    confusion_matrix: Vec<Vec<usize>>,
    overall_accuracy: f64,
    class_metrics: Vec<SklearnClassMetrics>,
    #[allow(dead_code)]
    feature_importances: Vec<f64>,
    importance_ranking: Vec<usize>,
}

#[derive(Deserialize)]
struct SklearnClassMetrics {
    #[allow(dead_code)]
    class: usize,
    precision: f64,
    recall: f64,
    f1: f64,
    #[allow(dead_code)]
    support: usize,
}

// ── Helpers ──────────────────────────────────────────────────────────────

fn load_reference() -> Reference {
    let path = concat!(env!("CARGO_MANIFEST_DIR"), "/../../cross_val/rf_reference.json");
    let data = fs::read_to_string(path)
        .expect("cross_val/rf_reference.json not found — run generate_rf_references.py first");
    serde_json::from_str(&data).expect("failed to parse rf_reference.json")
}

fn check(name: &str, pass: bool, detail: &str) {
    if pass {
        println!("  PASS  {name}: {detail}");
    } else {
        println!("  FAIL  {name}: {detail}");
    }
}

/// Compute Spearman rank correlation coefficient between two rank vectors.
fn spearman_rho(ranks_a: &[usize], ranks_b: &[usize]) -> f64 {
    let n = ranks_a.len() as f64;
    let d_squared_sum: f64 = ranks_a
        .iter()
        .zip(ranks_b)
        .map(|(&a, &b)| {
            let d = a as f64 - b as f64;
            d * d
        })
        .sum();
    1.0 - (6.0 * d_squared_sum) / (n * (n * n - 1.0))
}

fn main() {
    let ref_data = load_reference();

    let mut n_pass = 0u32;
    let mut n_fail = 0u32;

    let mut check_and_count = |name: &str, pass: bool, detail: &str| {
        check(name, pass, detail);
        if pass {
            n_pass += 1;
        } else {
            n_fail += 1;
        }
    };

    // ── 1. Train narcissus-rf with matching hyperparameters ──────────────

    println!("\n=== Training narcissus-rf ===");

    let config = RandomForestConfig::new(100)
        .unwrap()
        .with_max_features(MaxFeatures::Sqrt)
        .with_oob_mode(OobMode::Enabled)
        .with_seed(42);

    let result = config
        .fit(
            &ref_data.dataset.features,
            &ref_data.dataset.labels,
            &ref_data.dataset.feature_names,
        )
        .unwrap();

    let forest = result.forest();

    println!(
        "  Trained: {} trees, {} features, {} classes",
        forest.n_trees(),
        forest.n_features(),
        forest.n_classes()
    );

    // ── 2. Training accuracy comparison ──────────────────────────────────

    println!("\n=== Training accuracy ===");

    let rust_preds = forest.predict_batch(&ref_data.dataset.features).unwrap();
    let correct: usize = rust_preds
        .iter()
        .zip(&ref_data.dataset.labels)
        .filter(|&(&p, &l)| p == l)
        .count();
    let rust_accuracy = correct as f64 / ref_data.dataset.labels.len() as f64;

    let sklearn_accuracy = ref_data.sklearn_rf.train_accuracy;

    // Both should get high accuracy on well-separated data
    check_and_count(
        "rust accuracy high",
        rust_accuracy > 0.9,
        &format!("rust={rust_accuracy:.4}"),
    );
    check_and_count(
        "sklearn accuracy high",
        sklearn_accuracy > 0.9,
        &format!("sklearn={sklearn_accuracy:.4}"),
    );

    // Accuracy should be in the same ballpark (both are RF on same data)
    let accuracy_diff = (rust_accuracy - sklearn_accuracy).abs();
    check_and_count(
        "accuracy close",
        accuracy_diff < 0.1,
        &format!(
            "diff={accuracy_diff:.4}, rust={rust_accuracy:.4}, sklearn={sklearn_accuracy:.4}"
        ),
    );

    // ── 3. Prediction agreement ──────────────────────────────────────────

    println!("\n=== Prediction agreement ===");

    // Due to different RNG implementations, exact prediction match is unlikely.
    // Instead, check that predictions agree on most samples.
    let agree_count: usize = rust_preds
        .iter()
        .zip(&ref_data.sklearn_rf.predictions)
        .filter(|&(&r, &s)| r == s)
        .count();
    let agreement = agree_count as f64 / ref_data.dataset.labels.len() as f64;

    check_and_count(
        "prediction agreement",
        agreement > 0.8,
        &format!(
            "{agreement:.4} ({agree_count}/{})",
            ref_data.dataset.labels.len()
        ),
    );

    // ── 4. Feature importance ranking ────────────────────────────────────

    println!("\n=== Feature importance ranking ===");

    // Get narcissus-rf importance ranking
    let rust_importances = result.importances();

    // Build ranking: feature_index → rank (0-based)
    let mut rust_ranking = vec![0usize; ref_data.dataset.n_features];
    for feat in rust_importances {
        // Find the feature index by name
        if let Some(idx) = ref_data
            .dataset
            .feature_names
            .iter()
            .position(|n| n == &feat.name)
        {
            rust_ranking[idx] = feat.rank;
        }
    }

    // sklearn ranking (already 0-based indices sorted by importance)
    let mut sklearn_ranking = vec![0usize; ref_data.dataset.n_features];
    for (rank, &feat_idx) in ref_data.sklearn_rf.importance_ranking.iter().enumerate() {
        sklearn_ranking[feat_idx] = rank + 1; // 1-based
    }

    let rho = spearman_rho(&rust_ranking, &sklearn_ranking);
    check_and_count(
        "importance Spearman rho",
        rho > 0.6,
        &format!("rho={rho:.4}"),
    );

    // Check that top-3 features overlap
    let rust_top3: Vec<usize> = rust_importances
        .iter()
        .take(3)
        .filter_map(|f| {
            ref_data
                .dataset
                .feature_names
                .iter()
                .position(|n| n == &f.name)
        })
        .collect();
    let sklearn_top3 = &ref_data.sklearn_rf.importance_ranking[..3];
    let top3_overlap = rust_top3
        .iter()
        .filter(|idx| sklearn_top3.contains(idx))
        .count();
    check_and_count(
        "top-3 feature overlap",
        top3_overlap >= 2,
        &format!(
            "overlap={top3_overlap}/3, rust={rust_top3:?}, sklearn={sklearn_top3:?}"
        ),
    );

    // ── 5. Feature importances sum to 1.0 ────────────────────────────────

    println!("\n=== Feature importance normalization ===");

    let rust_imp_sum: f64 = rust_importances.iter().map(|f| f.importance).sum();
    check_and_count(
        "importances sum to 1.0",
        (rust_imp_sum - 1.0).abs() < 1e-10,
        &format!("sum={rust_imp_sum:.10}"),
    );

    let sklearn_imp_sum: f64 = ref_data.sklearn_rf.feature_importances.iter().sum();
    check_and_count(
        "sklearn importances sum to 1.0",
        (sklearn_imp_sum - 1.0).abs() < 1e-10,
        &format!("sum={sklearn_imp_sum:.10}"),
    );

    // ── 6. OOB accuracy ─────────────────────────────────────────────────

    println!("\n=== OOB accuracy ===");

    if let Some(oob) = result.oob_score() {
        check_and_count(
            "OOB accuracy reasonable",
            oob.accuracy > 0.8,
            &format!("oob_accuracy={:.4}", oob.accuracy),
        );
    } else {
        check_and_count("OOB computed", false, "OOB score was not computed");
    }

    // ── 7. Dataset shape sanity ──────────────────────────────────────────

    println!("\n=== Dataset shape ===");

    check_and_count(
        "n_samples",
        ref_data.dataset.n_samples == 300,
        &format!("n_samples={}", ref_data.dataset.n_samples),
    );
    check_and_count(
        "n_features",
        ref_data.dataset.n_features == 10,
        &format!("n_features={}", ref_data.dataset.n_features),
    );
    check_and_count(
        "n_classes",
        ref_data.dataset.n_classes == 3,
        &format!("n_classes={}", ref_data.dataset.n_classes),
    );

    // ── 8. Cross-validation comparison ──────────────────────────────────

    println!("\n=== Cross-validation (5-fold) ===");

    let cv = CrossValidation::new(5).unwrap().with_seed(42);
    let cv_config = RandomForestConfig::new(100)
        .unwrap()
        .with_max_features(MaxFeatures::Sqrt)
        .with_seed(42);

    let cv_result = cv
        .evaluate(
            &cv_config,
            &ref_data.dataset.features,
            &ref_data.dataset.labels,
            &ref_data.dataset.feature_names,
        )
        .unwrap();

    let sklearn_cv = &ref_data.sklearn_cv;

    // Check: CV n_folds matches
    check_and_count(
        "cv n_folds",
        cv_result.n_folds == sklearn_cv.n_folds,
        &format!("rust={}, sklearn={}", cv_result.n_folds, sklearn_cv.n_folds),
    );

    // Check: CV mean accuracy close
    let cv_mean_diff = (cv_result.mean_accuracy - sklearn_cv.mean_accuracy).abs();
    check_and_count(
        "cv mean accuracy close",
        cv_mean_diff < 0.05,
        &format!(
            "diff={cv_mean_diff:.4}, rust={:.4}, sklearn={:.4}",
            cv_result.mean_accuracy, sklearn_cv.mean_accuracy
        ),
    );

    // Check: CV mean accuracy high (well-separated data)
    check_and_count(
        "cv mean accuracy high",
        cv_result.mean_accuracy > 0.85,
        &format!("rust_mean={:.4}", cv_result.mean_accuracy),
    );

    // Check: CV std accuracy low (consistent folds on well-separated data)
    check_and_count(
        "cv std accuracy low",
        cv_result.std_accuracy < 0.10,
        &format!(
            "rust_std={:.4}, sklearn_std={:.4}",
            cv_result.std_accuracy, sklearn_cv.std_accuracy
        ),
    );

    // Check: Aggregated confusion matrix accuracy close
    let rust_cm_accuracy = cv_result.confusion_matrix.accuracy();
    let cm_acc_diff = (rust_cm_accuracy - sklearn_cv.overall_accuracy).abs();
    check_and_count(
        "cv confusion matrix accuracy close",
        cm_acc_diff < 0.05,
        &format!(
            "diff={cm_acc_diff:.4}, rust={rust_cm_accuracy:.4}, sklearn={:.4}",
            sklearn_cv.overall_accuracy
        ),
    );

    // Check: Per-class metrics close (precision, recall, F1 within 0.10)
    let rust_class_metrics = cv_result.confusion_matrix.class_metrics();
    let mut class_metrics_ok = true;
    for (rust_m, sklearn_m) in rust_class_metrics.iter().zip(&sklearn_cv.class_metrics) {
        let p_diff = (rust_m.precision - sklearn_m.precision).abs();
        let r_diff = (rust_m.recall - sklearn_m.recall).abs();
        let f1_diff = (rust_m.f1 - sklearn_m.f1).abs();
        if p_diff > 0.10 || r_diff > 0.10 || f1_diff > 0.10 {
            class_metrics_ok = false;
            println!(
                "    class {}: p_diff={p_diff:.4}, r_diff={r_diff:.4}, f1_diff={f1_diff:.4}",
                rust_m.class
            );
        }
    }
    check_and_count(
        "cv per-class metrics close",
        class_metrics_ok,
        &format!(
            "{}",
            rust_class_metrics
                .iter()
                .map(|m| format!(
                    "c{}:p={:.3}/r={:.3}/f1={:.3}",
                    m.class, m.precision, m.recall, m.f1
                ))
                .collect::<Vec<_>>()
                .join(", ")
        ),
    );

    // Check: CV feature importance ranking correlation
    let mut rust_cv_ranking = vec![0usize; ref_data.dataset.n_features];
    for feat in &cv_result.feature_importances {
        if let Some(idx) = ref_data
            .dataset
            .feature_names
            .iter()
            .position(|n| n == &feat.name)
        {
            rust_cv_ranking[idx] = feat.rank;
        }
    }
    let mut sklearn_cv_ranking = vec![0usize; ref_data.dataset.n_features];
    for (rank, &feat_idx) in sklearn_cv.importance_ranking.iter().enumerate() {
        sklearn_cv_ranking[feat_idx] = rank + 1;
    }
    let cv_rho = spearman_rho(&rust_cv_ranking, &sklearn_cv_ranking);
    check_and_count(
        "cv importance Spearman rho",
        cv_rho > 0.6,
        &format!("rho={cv_rho:.4}"),
    );

    // Check: CV importances sum to 1.0
    let cv_imp_sum: f64 = cv_result
        .feature_importances
        .iter()
        .map(|f| f.importance)
        .sum();
    check_and_count(
        "cv importances sum to 1.0",
        (cv_imp_sum - 1.0).abs() < 1e-10,
        &format!("sum={cv_imp_sum:.10}"),
    );

    // ── Summary ─────────────────────────────────────────────────────────

    println!("\n{}", "=".repeat(60));
    println!("  {n_pass} passed, {n_fail} failed\n");

    if n_fail > 0 {
        std::process::exit(1);
    }
}
